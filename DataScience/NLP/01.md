# Transformer架构介绍

## Transformer模型架构入门

### NLP背景

1. 马尔可夫，随机过程，1902年证明可以使用马尔可夫链的最后一个元素来预测下一个元素
2. 1948年，香农《The Mathematical Theory of Communication》，通信模型的数学基础
3. 1950年，图灵《Computational machinery and Intelligence》
4. 1954年，Georgetown-IBM实验室，通过定义规则实现一些俄语句子到英语的翻译
5. 1982年，霍普克罗夫引入RNN，存储序列的持久状态，1974年写下著作《The Existence of Persistent States in the Brain》
6. 1980年代，Yann Le Cun将CNN引入文本序列的处理，90年代提出LeNet-5模型是很多NLP模型的基础
7. 注意力（Attention）：窥视（Peeking）序列中的其他元素，而不仅仅关注最后一个

### Attention is all you need

2017年Google研究团队发表的《Attention is all you need》提出了初始的Transformer模型：

![架构](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png "架构图")

编码器和解码器均是6层堆栈，从架构图中可以发现循环已经被弃用，l层的输出就是l+1层的输入。注意力机制取代了循环，是一个词组对词组的操作，是在词组间进行点积运算（而循环需要操作的次数随词组之间距离的增加而增加），利用此机制将发现每个词组与序列中所有词组之间的关联关系。注意力机制往往会并行运行多个注意力（Multi-head Attention），从而对序列进行更广泛的深入学习。

### Encoder

初始模型中有6层的encoder堆栈，每层encoder由Multi-head Attention Layer（多头注意力机制子层）和Feed Forward Layer（基于位置的全连接前馈子层）组成，子层间由归一化层连接，每个归一化的输出为LayerNormalization(x+subLayer(x))，其中x为未处理的输入数据，从而确保位置等关键信息不会丢失。

输入嵌入子层只与第一层编码器层连接，整个编码部分的模型维度是确定的，初始模型指定$$d_{model}=512$$

示例句子："The black cat sat on the couch and the brown dog slept on the rug."

1. 输入嵌入子层：将输入标记转换为d_model维度（512）维的向量（将每一个单词转换为d维的向量表示（嵌入）），与其他标准转换模型类似，如Word2Vec的skip-gram架构。如对black和brown都将生成一个512维的向量，可以使用余弦相似度来评估这两个嵌入向量的相似度。
2. 位置编码：使用正余弦函数进行位置编码，这里奇数位置使用余弦函数进行编码，偶数位置使用正弦函数进行编码，函数定义方式可以有所不同，如前256个使用正弦编码，其中pos为单词在句子中的位置，从1开始，如black的pos=2，而brown的pos=10：

$$
PE(pos_{2i})=\sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\
PE(pos_{2i+1})=\cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

3. 将位置编码与词嵌入向量结合：如可以直接相加：

$$pc(black)=y_1+pe(2)$$

可以对$$y_1$$进行处理保证词向量信息的传递，如：

$$y_1 = y_1 \sqrt{d_{model}}$$

4. 多头注意力子层：8个头部+归一化
