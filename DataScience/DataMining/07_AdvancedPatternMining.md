# 07 高级模式挖掘

## 7.1 一个路线图

+ 模式挖掘研究
  + 模式和规则的类型
    + 基本模式（频繁模式、关联规则、极大模式（闭模式）、生成元）
    + 多层和多维模式（多层、多维、连续数据（基于离散化或者基于统计））
    + 扩充的模式（近似模式、不确定模式、压缩模式、稀有模式、负模式、高维和巨型模式）
  + 挖掘方法
    + 基本挖掘方法（多候选产生（Apriori、划分、抽样等）、模式增长（FP-Growth、HMine、FPMax、Close+等）、垂直格式（Eclat、CHARM等））
    + 挖掘感兴趣的模式（兴趣度、基于约束、相关规则、异常规则）
    + 分布、并行和增量的（分布/并行挖掘、增量挖掘、流模式）
  + 扩充和应用
    + 扩充的数据类型（序列和时间序列模式、结构模式（如图、格、树、序列、集合）、空间模式、媒体模式、网络模式）
    + 应用（基于模式的分类、聚类、语义注释，协同过滤、保护隐私）

## 7.2 多层、多维空间中的模式挖掘

### 7.2.1 多层关联规则

在较高的抽象层发现的强关联规则可能只是常识，在较低的抽象层发现的规则可能只是高抽象层的平凡特化。

$\Rightarrow$关注如何在多个抽象层进行灵活的模式挖掘。

先对数据进行概念分层，在多个抽象层上挖掘产生的关联规则称为多层关联规则，一般采用自顶向下策略有高度抽象层向下挖掘。在支持度-置信度框架下，可以采用一致的最小支持度，也可以采用递减支持度（较低抽象层使用较小的支持度阈值）、基于项或分组的最小支持度（通常使用所有组的最小支持度阈值，以避免过滤掉有价值的模式）。

如果一个规则的支持度和置信度接近于其祖先规则的支持度和置信度，则称这个规则是冗余的。

### 7.2.2 多维关联规则

涉及两个或多个维或谓词的关联规则被称为多维关联规则，具有不重复谓词的关联规则被称为维间（interdimension）关联规则，否则称为混合维（hybrid-dimension）关联规则，如：

$age(X,"20..29")∧{occupation(X,"student")}\Rightarrow{buys(X,"laptap")}$

### 7.2.3 量化关联规则

即对量化数据进行规则挖掘（一般的关联挖掘基于nominal量表），几种典型方法：数据立方体方法（基于数据的离散化）、基于聚类的方法和揭示异常行为的统计学方法。

### 7.2.4 挖掘稀有模式和负模式

稀有模式（非频繁模式）是支持度远低于用户指定的最小支持阈值的模式。

如果项集$X$和$Y$是频繁的，但很少一起出现（$sup(X\bigcup Y)<sup(X)\times sup(Y)$），则称项集$X$和$Y$是负相关的，模式$X\bigcup Y$是负相关模式。（只是一种定义方法，但这种定义不是零不变的，一般的判定标准是：$(P(X|Y)+P(Y|X))/2<\epsilon$，其中$\epsilon$是负模式阈值）

## 7.3 基于约束的频繁模式挖掘

规则挖掘过程中可能会从数据集中发现很多规则，其中大部分是不感兴趣的。一种启发式方法是将用户的期望或直观作为限制搜索空间的约束条件，即基于约束的挖掘。一般而言，约束包括：

+ 知识规则约束：待挖掘的知识类型
+ 数据约束：指定任务相关的数据集
+ 维/层约束：指定挖掘种所使用的数据维度、抽象层、概念分层的层次
+ 兴趣度约束：规则的统计度量阈值
+ 规则约束：指定挖掘规则的形式或条件（如前件、后件的谓词个数，属性之间的联系等）

（本节主要讨论规则约束）

### 7.3.1 元规则指导规则挖掘

元规则是形如$P_1∧P_2∧..∧P_l\Rightarrow Q_1∧Q_2∧..∧Q_r$的规则模板，其中$P_i、Q_j$是被例示的谓词或谓词变量。元规则使得用户可以说明他们感兴趣的规则的语法形式。

### 7.3.2 基于约束的模式产生：模式空间剪枝和数据空间剪枝

模式空间剪枝：检查候选模式，确定模式是否可以被剪掉（如果在剩下的挖掘过程中其超模式不可能产生，则可以剪掉这个模式）

数据搜索空间剪枝：检查数据集，确定特定的数据片段在剩下的挖掘过程中是否对其后的可满足模式有所贡献。如果不能，则剪去。

模式剪枝规则约束可以分为五类：

+ 反单调的：如果一个项集不满足该规则约束，则它的任何超集也不可能满足该规则约束。
+ 单调的：如果一个项集满足该规则约束，则它的任何超集必然满足该规则约束。
+ 简洁的：可以枚举并且仅枚举确保满足该约束的所有项集。
+ 可转变的
+ 不可转变的

数据空间剪枝约束可以分为两类：

+ 数据简洁的：在模式挖掘过程开始时对不可能满足该约束的数据子集剪枝。
+ 反单调的：挖掘过程中，如果基于当前模式，一个数据项不满足数据的反单调约束，则可以剪掉

## 7.4 挖掘高维数据和巨型模式

高维数据的挖掘的两个思路：

+ 使用垂直格式挖掘，如基因的分析
+ 开发新的挖掘算法，即使用模式融合挖掘巨型模式（conlossal pattern）

一种挖掘巨型模式的方法：模式融合（Pattern-Fusion），以有限的宽度遍历树，模式增长基于与其他模式凝聚。

核模式，对于模式$\alpha$，项集$\beta\subseteq\alpha$
称为$\alpha$的$\tau-$核模式，如果$|\frac{D_\alpha}{D_\beta}|\ge\tau, 0<\tau\le 1$，其中$|D_\alpha|$是数据库$D$中包含$\alpha$的模式数，$\tau$称为核比率。模式$\alpha$是$(d,\tau)-robust$的，其中$d=max_{\beta}\{|\alpha|-|\beta||\beta\subseteq\alpha，并且\beta是\alpha的\tau-模式\}$，巨型模式的$d$更大，鲁棒性更强。

模式融合方法包括两个阶段：

+ 池初始化：初始化一个短频繁模式的初始池；
+ 迭代的模式融合：在搜索空间内凝聚子模式。

## 7.5 挖掘压缩或近似模式

### 7.5.1 通过模式聚类挖掘压缩模式

使用最小支持度阈值挖掘规则的效果有限——若阈值太低则产生的规则太多；若阈值太高则可能产生的规则是常识。挖掘频繁模式的压缩集合或近似集合可以解决这个问题。

模式压缩可以通过模式聚类实现，即对频繁模式进行聚类（使用$\delta-$簇紧密型度量），代表模式仅从每个簇中选取，从而提供频繁模式的一个压缩版本。因为闭频繁模式的集合是原频繁模式的无损压缩，因此在闭模式集合上发现代表模式是一种选择。设$P_1、P_2$是两个闭模式，其支持的事务集分别为$T(P_1)、T(P_2)$，则$P_1、P_2$之间的模式距离$Pat\_Dist(P_1,P_2)=1-\frac{T(P_1\bigcap T(P_2))}{T(P_1\bigcup T(P_2))}$。

考虑同簇中代表模式的选取：给定两个模式$A、B$，若$O(B)\subset O(A)$，其中$O(A)$表示模式$A$的项集，则称$B$可以被$A$表达。则对同一个簇中的模式$P_1,P_2,..,P_k$而言，则代表模式$P_r$应当满足$\bigcup_{i=1}^{k}{O(P_i)}\subseteq{O(P_r)}$。

考虑模式的聚类：一般的聚类方法可能不适用于模式的聚类，这里使用了所谓$\delta-$簇的聚类方法，即对模式$P$和$P'$而言，如果$O(P)\subseteq O(P')$，并且$Pat\_Dist(P,P')\le\delta$，则称$P$被$P'\delta$覆盖。如果在一个存在一个代表模式$P_r$使得该集合中的每个模式$P$都被$P_r\delta-$覆盖，则这个模式集就是一个$\delta-$簇。由此概念可知，一个模式可能属于多个簇，使用$\delta-$簇只需要考虑每个模式与代表模式之间的距离，即$Pat\_Dist(P,P_r)=1-\frac{T(P_r)}{T(P)}$。

为了得到更简洁的表示，我们允许代表模式的支持度稍小于$min\_sup$：对于代表模式$P_r$，假设其支持度为$k$，则有$\delta\ge 1-\frac{T(P_r)}{T(p)}\ge 1-\frac{k}{min\_sup}$。可得：$k\ge(1-\delta)\times{min\_sup}$，使用这个稍小的值作为代表模式的最小支持度阈值。

则模式压缩问题的定义如下：给定一个事务数据库、最小支持度阈值和聚类质量度量$\delta$，模式压缩问题是找到一个代表性的集合$R$，使得对于每个频繁项集$P$，存在代表模式$P_r\in R$，$P_r$覆盖$P$，且$|P|$最小化。

找出代表模式的最小集合是$NP$困难问题。

### 7.5.2 提取冗余的top-k模式

挖掘top-k个最频繁模式是一种减少挖掘返回的模式数量的策略。即用户更愿意得到k个最感兴趣的模式，不仅具有高显著性，而且具有低冗余的k个代表模式的小集合称为感知冗余的top-k模式。

模式$p$的显著性度量$S(p)$将其映射到一个实数值。联合显著性$S(p,q)$与共同显著性$S(p|q)$的关系为$S(p|q)=S(p,q)-S(q)$，冗余性度量$R(p,q)=S(p)+S(q)-S(p,q)$，且应当满足$0\le R(p,q)\le min\{S(p),S(q)\}$。

发现感知冗余的top-k问题可以转换为发现最大化边缘显著性的k-模式集问题，该问题在信息检索领域已被研究透彻。（不做深究）

## 7.6 模式探索与应用

本节考察如何自动产生频繁模式的语义注解。

对模式$p$的语境建模，有如下考虑：

+ 语境单元（Context Unit）是数据库D的基本对象，其携带语义信息，并且至少与一个频繁模式$p$一起出现在D的事务中。
+ 模式$p$的语境（Context）是从数据库中挑选的加权语境单元的集合（称为语境指示符，context indicator），其携带语义信息，且与频繁模式$p$一起出现，可以用向量表示为$C(p)=<w(u_1),w(u_2),..,w(u_n)>$，其中$w(u_i)$是项$u_i$的权重函数。

则定义语义模式注解的基本任务为：

+ 选择语境单元，并对每个单元设计强度权重，对频繁模式的语境建模；
+ 为两个模式的语境、一个事务和一个模式的语境设计相似性度量；
+ 对给定的频繁模式，提取最显著的语境提示符、代表事务和语义相似模式，构建注解。

则依次考虑：

+ 选择哪些语境单元作为语境提示符？选择最重要的、非冗余的频繁模式，或者进行了模式压缩的闭模式；
+ 如何设置权重？互信息是一种度量的权重函数，度量一个随机变量能推断另一个随机变量多少信息，给定两个频繁模式$p_{\alpha}、p_{\beta}$，令随机变量$X=\{0,1\}、Y=\{0,1\}$是两个随机变量，分别代表$p_{\alpha}、p_{\beta}$的出现，则互信息$I(X;Y)=\sum_{x\in{X}}\sum_{y\in{Y}}{P(x,y)log\frac{P(x,y)}{P(x)P(y)}}$，其中$P(x=1,y=1)=\frac{|D_{\alpha}\bigcap{D_\beta}|}{|D|},P(x=0,y=1)=\frac{|D_\beta|-|D_\alpha\bigcap D_\beta|}{|D|},P(x=1,y=0)=\frac{|D_\alpha|-|D_\alpha\bigcap D_\beta|}{|D|},P(x=0,y=0)=\frac{|D|-|D_\alpha\bigcup D_\beta|}{|D|},$

基于以上原则，即可产生语义注解。

再简单介绍一下模式挖掘的应用：

+ 密集型应用中，模式挖掘作为预处理广泛地应用于噪声过滤和数据清理中；
+ 有助于发现隐藏在数据中的固有结构和簇，从而帮助进行分类和聚类过程；
+ 对时空间数据分析、媒体数据等的分析很有用；
+ 用作基本的索引结构；
+ 用于推荐系统

……